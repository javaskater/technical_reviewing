# p 44
## I stopped at [implementing the examples in Python](https://github.com/KrisNguyen135/bayesian-optimization-in-action/blob/main/CH02/01%20-%20Gaussian%20processes.ipynb)
* I put them on my Jupyter account at the [CNAM](https://www.cnam.eu/site-en/) 
  * (I still have problem with the local installation of the python libraries)
# 45
* I add print instructions to the code to understand squeeze (suppression of a dim) and unsqueeze (adding a dim)
```python

xs = torch.linspace(-3, 3, 101).unsqueeze(1) # add a dimension 1 (3 lines one columne
print(f"[main-4] size of xs after unsqueeze {xs.shape}") # we pass a matrix/tensor of 101 lines and one col
ys = forrester_1d(xs) # ys is 

plt.figure(figsize=(8, 6))

plt.plot(xs, ys, label="objective", c="r") # when we plot xs is unsqueezed byt the ys are squuezed
plt.scatter(train_x, train_y, marker="x", c="k", label="observations")

plt.legend(fontsize=15);
```
# 48
* The [Multivariate Normal function formula](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) as a product from covariance matrix Sigma of points and mean vector Mu
  *  here the rank is the dimension because Sigma has an inverted Matrix
* I have a problem here **gpytorch** is not known at the CNAM Jupyter
# 49
* Note at page end: why use no_grad ? because we don't wand any gradient descent with our predictions
# 50
* CI stands for **Confidence Interval**
# 52
* we didn't repeat (but it was implied)
```python
plt.ylim(-3, 3)
```
* GP stands for **Gaussian Process**
* [Gaussian Likehood](https://docs.gpytorch.ai/en/v1.6.0/likelihoods.html) uses a Sigma scala and an Identity vector
# 53
## Cell 17
* I added some printing/debuggins to understand better the mesgrid
  * and the transpose function
```python
grid_x = torch.linspace(-3, 3, 101)

grid_x1, grid_x2 = torch.meshgrid(grid_x, grid_x, indexing="ij")
print(f"[main-13] {grid_x1.shape} - {grid_x2.shape}, flatten {grid_x1.flatten().shape} - {grid_x2.flatten().shape}")
xs = torch.vstack([grid_x1.flatten(), grid_x2.flatten()])
print(f"[main-13] xs before transpose {xs.shape}")
xs = xs.transpose(-1, -2)
print(f"[main-13] xs {xs.shape}")
```
# 54
## Cell 17
* I don't know why it does a transpose
* from this [Pytorch documentation about reshape](https://docs.pytorch.org/docs/stable/generated/torch.reshape.html)
```python
a = torch.arange(4.)
print(a)
b = torch.reshape(a, (2, 2))
print(b)
print(b.transpose(-1, -2))
```  
# 55, 56 Exercise
* TODO (see [Solution on Github](https://github.com/KrisNguyen135/bayesian-optimization-in-action/blob/main/CH02/02%20-%20Exercise.ipynb))
* I still does not understand the transpose when plotting the results in 2D